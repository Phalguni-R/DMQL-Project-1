import ast
import os
import time

import pandas as pd
from dotenv import load_dotenv
from sqlalchemy import create_engine, text

# Load database credentials from .env file
load_dotenv()
DB_USER = os.getenv("DB_USER")
DB_PASSWORD = os.getenv("DB_PASSWORD")
DB_HOST = os.getenv("DB_HOST")
DB_PORT = os.getenv("DB_PORT")
DB_NAME = os.getenv("DB_NAME")

if not all([DB_USER, DB_PASSWORD, DB_HOST, DB_PORT, DB_NAME]):
    print("ERROR: Missing database credentials in .env file")
    exit()

DATABASE_URL = f"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}"
CLEANED_DATA_DIR = 'fma_metadata_cleaned'


def insert_data(engine, table_name, records):
    """
    Insert a list of records into a database table.
    If a record already exists, skip it (ON CONFLICT DO NOTHING).
    """
    if not records:
        print(f"  No data to insert into {table_name}")
        return 0

    columns = records[0].keys()
    sql = text(f"""
        INSERT INTO "{table_name}" ({', '.join(f'"{col}"' for col in columns)}) 
        VALUES ({', '.join(f':{col}' for col in columns)})
        ON CONFLICT DO NOTHING;
    """)

    with engine.connect() as connection:
        result = connection.execute(sql, records)
        connection.commit()
        return result.rowcount


def load_cleaned_data():
    """Load all the cleaned CSV files into memory."""
    data_path = os.path.join(os.getcwd(), CLEANED_DATA_DIR)
    print(f"Loading clean data from {data_path}...")

    file_names = ['genres', 'artists', 'albums', 'tracks', 'echonest']
    data = {}

    for name in file_names:
        file_path = os.path.join(data_path, f"clean_{name}.csv")
        data[name] = pd.read_csv(file_path)

    return data


def create_lookup_tables(engine, clean_data):
    """
    Create lookup tables for Engineers, Lyricists, Labels, and Licenses.
    These are extracted from the main CSV files and normalized into separate tables.
    """
    print("\nCreating lookup tables...")

    # Engineers come from the albums file
    print("  Processing Engineers...")
    engineer_names = (clean_data['albums']['album_engineer']
                      .dropna()
                      .astype(str)
                      .str.split(r'[,&\n]')
                      .explode()
                      .str.strip()
                      .unique())
    insert_data(engine, 'Engineers', [{'engineer_name': name} for name in engineer_names])

    # Lyricists come from the tracks file
    print("  Processing Lyricists...")
    lyricist_names = (clean_data['tracks']['track_lyricist']
                      .dropna()
                      .astype(str)
                      .str.split(r'[,&\n]')
                      .explode()
                      .str.strip()
                      .unique())
    insert_data(engine, 'Lyricists', [{'lyricist_name': name} for name in lyricist_names])

    # Labels come from the artists file
    print("  Processing Labels...")
    label_names = (clean_data['artists']['artist_associated_labels']
                   .dropna()
                   .astype(str)
                   .str.split(r'[,&\n]')
                   .explode()
                   .str.strip()
                   .unique())
    insert_data(engine, 'Labels', [{'label_name': name} for name in label_names])

    # Licenses come from the tracks file
    print("  Processing Licenses...")
    license_titles = clean_data['tracks']['license_title'].dropna().unique()
    insert_data(engine, 'Licenses', [{'license_title': title} for title in license_titles])

    # Now read back the IDs that were auto-generated by the database
    # We'll need these IDs to link records in other tables
    lookups = dict()
    lookups['engineers'] = \
        pd.read_sql('SELECT engineer_id, engineer_name FROM "Engineers"', engine).set_index('engineer_name')[
            'engineer_id'].to_dict()
    lookups['lyricists'] = \
        pd.read_sql('SELECT lyricist_id, lyricist_name FROM "Lyricists"', engine).set_index('lyricist_name')[
            'lyricist_id'].to_dict()
    lookups['labels'] = pd.read_sql('SELECT label_id, label_name FROM "Labels"', engine).set_index('label_name')[
        'label_id'].to_dict()
    lookups['licenses'] = \
        pd.read_sql('SELECT license_id, license_title FROM "Licenses"', engine).set_index('license_title')[
            'license_id'].to_dict()

    return lookups


def insert_genres(engine, genres_df):
    """
    Insert genres into the database in the correct order.
    Since genres can have parent genres, we need to insert parents before children.
    """
    print("\nInserting Genres...")

    # Prepare the data
    genres = genres_df.copy()
    genres.rename(columns={'genre_title': 'genre_name', 'genre_parent_id': 'parent_id'}, inplace=True)
    genres['parent_id'] = pd.to_numeric(genres['parent_id'], errors='coerce').astype('Int64')

    # Track which genres we've successfully inserted
    inserted_ids = set()
    remaining_genres = genres.to_dict(orient='records')

    # Keep trying to insert genres until we can't insert anymore
    # A genre can only be inserted if its parent is already in the database
    while remaining_genres:
        ready_to_insert = []

        for genre in remaining_genres:
            # A genre is ready if it has no parent OR its parent is already inserted
            has_no_parent = pd.isna(genre['parent_id'])
            parent_already_inserted = genre['parent_id'] in inserted_ids

            if has_no_parent or parent_already_inserted:
                ready_to_insert.append(genre)

        # If we couldn't find any genres to insert, we have a problem
        if not ready_to_insert:
            print(f"  WARNING: {len(remaining_genres)} genres have missing or circular parent references")
            print("  These genres will be skipped")
            break

        # Insert this batch of genres
        insert_data(engine, 'Genres', ready_to_insert)

        # Update our tracking
        for genre in ready_to_insert:
            inserted_ids.add(genre['genre_id'])

        # Remove inserted genres from remaining list
        remaining_genres = [g for g in remaining_genres if g['genre_id'] not in inserted_ids]

    print(f"  Inserted {len(inserted_ids)} genres")


def insert_artists(engine, artists_df):
    """Insert all artists into the database."""
    print("\nInserting Artists...")

    artists = artists_df[['artist_id', 'artist_name', 'artist_handle', 'artist_website',
                          'artist_active_year_begin', 'artist_favorites']].copy()

    # Convert numeric columns properly (some might be strings or have bad data)
    artists['artist_active_year_begin'] = pd.to_numeric(artists['artist_active_year_begin'], errors='coerce').astype(
        'Int64')
    artists['artist_favorites'] = pd.to_numeric(artists['artist_favorites'], errors='coerce').astype('Int64')

    count = insert_data(engine, 'Artists', artists.to_dict(orient='records'))
    print(f"  Inserted {count} artists")


def insert_albums(engine, albums_df, artists_df):
    """Insert all albums into the database."""
    print("\nInserting Albums...")

    # We need to link albums to artists by ID, but the CSV has artist names
    # So first we create a mapping from artist name to artist ID
    artist_name_to_id = pd.Series(artists_df.artist_id.values, index=artists_df.artist_name).to_dict()

    albums = albums_df.copy()
    albums['artist_id'] = albums['artist_name'].map(artist_name_to_id)

    # Select only the columns we need for the database
    albums = albums[['album_id', 'album_title', 'album_type', 'album_tracks',
                     'album_date_released', 'album_listens', 'album_favorites', 'artist_id']].copy()

    # Convert date column
    albums['album_date_released'] = pd.to_datetime(albums['album_date_released'], errors='coerce')

    # Convert to records and handle null dates (Pandas NaT becomes None for SQL)
    records = albums.to_dict(orient='records')
    for record in records:
        if pd.isna(record['album_date_released']):
            record['album_date_released'] = None

    count = insert_data(engine, 'Albums', records)
    print(f"  Inserted {count} albums")


def insert_tracks(engine, tracks_df, license_map):
    """Insert all tracks into the database."""
    print("\nInserting Tracks...")

    tracks = tracks_df.copy()
    tracks.dropna(subset=['track_title'], inplace=True)

    # Link tracks to their license using the lookup map
    tracks['license_id'] = tracks['license_title'].map(license_map)

    # Select the columns we need
    tracks = tracks[['track_id', 'track_title', 'track_language_code', 'track_listens',
                     'track_favorites', 'track_url', 'track_duration', 'track_bit_rate',
                     'track_date_recorded', 'album_id', 'artist_id', 'license_id']].copy()

    # Convert numeric columns
    numeric_cols = ['track_listens', 'track_favorites', 'track_bit_rate', 'album_id', 'artist_id', 'license_id']
    for col in numeric_cols:
        tracks[col] = pd.to_numeric(tracks[col], errors='coerce').astype('Int64')

    # Convert date column
    tracks['track_date_recorded'] = pd.to_datetime(tracks['track_date_recorded'], errors='coerce')

    # Handle null dates
    records = tracks.to_dict(orient='records')
    for record in records:
        if pd.isna(record['track_date_recorded']):
            record['track_date_recorded'] = None

    count = insert_data(engine, 'Tracks', records)
    print(f"  Inserted {count} tracks")


def create_linking_tables(engine, clean_data, lookups):
    """
    Create many-to-many relationship tables.
    These link tracks to genres, albums to engineers, artists to labels, etc.
    """
    print("\nCreating relationship links...")

    # Link albums to engineers
    print("  Linking albums to engineers...")
    album_engineers = clean_data['albums'][['album_id', 'album_engineer']].dropna()
    album_engineers['engineer_name'] = album_engineers['album_engineer'].astype(str).str.split(r'[,&\n]')
    album_engineers = album_engineers.explode('engineer_name')
    album_engineers['engineer_name'] = album_engineers['engineer_name'].str.strip()
    album_engineers['engineer_id'] = album_engineers['engineer_name'].map(lookups['engineers'])

    records = album_engineers[['album_id', 'engineer_id']].dropna().astype(int).drop_duplicates().to_dict(
        orient='records')
    insert_data(engine, 'AlbumEngineers', records)

    # Link artists to labels
    print("  Linking artists to labels...")
    artist_labels = clean_data['artists'][['artist_id', 'artist_associated_labels']].dropna()
    artist_labels['label_name'] = artist_labels['artist_associated_labels'].astype(str).str.split(r'[,&\n]')
    artist_labels = artist_labels.explode('label_name')
    artist_labels['label_name'] = artist_labels['label_name'].str.strip()
    artist_labels['label_id'] = artist_labels['label_name'].map(lookups['labels'])

    records = artist_labels[['artist_id', 'label_id']].dropna().astype(int).drop_duplicates().to_dict(orient='records')
    insert_data(engine, 'ArtistLabels', records)

    # Link tracks to lyricists
    print("  Linking tracks to lyricists...")
    track_lyricists = clean_data['tracks'][['track_id', 'track_lyricist']].dropna()
    track_lyricists['lyricist_id'] = track_lyricists['track_lyricist'].map(lookups['lyricists'])

    records = track_lyricists[['track_id', 'lyricist_id']].dropna().astype(int).drop_duplicates().to_dict(
        orient='records')
    insert_data(engine, 'TrackLyricists', records)

    # Link tracks to genres (this one is more complex because genres are stored as a list)
    print("  Linking tracks to genres...")
    valid_genre_ids = set(pd.read_sql('SELECT genre_id FROM "Genres"', engine)['genre_id'])
    track_genre_records = []

    for _, row in clean_data['tracks'][['track_id', 'track_genres']].dropna().iterrows():
        try:
            # The track_genres column contains a string representation of a list
            # We need to parse it and extract the genre IDs
            genres_list = ast.literal_eval(row['track_genres'])
            for genre in genres_list:
                genre_id = int(genre.get('genre_id'))
                # Only link to genres that actually exist in our database
                if genre_id in valid_genre_ids:
                    track_genre_records.append({'track_id': row['track_id'], 'genre_id': genre_id})
        except:
            # If we can't parse the genres, just skip this track
            continue

    records = pd.DataFrame(track_genre_records).drop_duplicates().to_dict(orient='records')
    insert_data(engine, 'TrackGenres', records)


def insert_audio_features(engine, echonest_df):
    """
    Insert audio and social features for tracks.
    These come from the Echonest analysis and include things like danceability, energy, etc.
    """
    print("\nInserting audio and social features...")

    # Only insert features for tracks that actually exist in the database
    valid_track_ids = set(pd.read_sql('SELECT track_id FROM "Tracks"', engine)['track_id'])
    features = echonest_df[echonest_df['track_id'].isin(valid_track_ids)]

    # Split into two tables: Audio features and Social features
    audio_cols = ['track_id', 'acousticness', 'danceability', 'energy', 'instrumentalness',
                  'liveness', 'speechiness', 'tempo', 'valence']
    social_cols = ['track_id', 'artist_discovery', 'artist_familiarity', 'artist_hotttnesss',
                   'song_currency', 'song_hotttnesss']

    insert_data(engine, 'Audio', features[audio_cols].to_dict(orient='records'))
    insert_data(engine, 'Social', features[social_cols].to_dict(orient='records'))


def main():
    """Run the complete data ingestion pipeline."""

    st = time.time()
    # Connect to the database
    try:
        engine = create_engine(DATABASE_URL)
        with engine.connect() as connection:
            print("Connected to database successfully\n")
    except Exception as e:
        print(f"ERROR: Could not connect to database. {e}")
        return

    # Load all the cleaned data files
    try:
        clean_data = load_cleaned_data()
    except FileNotFoundError as e:
        print(f"ERROR: {e}")
        print(f"Make sure the '{CLEANED_DATA_DIR}' folder exists with all cleaned CSV files")
        return

    # Now insert data in the correct order to respect foreign key relationships

    # Step 1: Create independent lookup tables and get their ID mappings
    lookups = create_lookup_tables(engine, clean_data)

    # Step 2: Insert core entities
    insert_genres(engine, clean_data['genres'])
    insert_artists(engine, clean_data['artists'])

    # Step 3: Insert entities that depend on the core entities
    insert_albums(engine, clean_data['albums'], clean_data['artists'])
    insert_tracks(engine, clean_data['tracks'], lookups['licenses'])

    # Step 4: Create many-to-many relationship links
    create_linking_tables(engine, clean_data, lookups)

    # Step 5: Insert audio and social features
    insert_audio_features(engine, clean_data['echonest'])

    print("\nâœ“ All data has been successfully ingested into the database!")
    engine.dispose()
    en = time.time()

    print(f"\n Total time taken : {en - st}")


if __name__ == "__main__":
    main()
